{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report, precision_recall_fscore_support\n",
    "import csv\n",
    "import huggingface_hub\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "huggingface_hub.login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed before shuffling\n",
    "random.seed(31)\n",
    "np.random.seed(31)\n",
    "torch.manual_seed(31)\n",
    "df = pd.read_csv('../data/unpreprocessed/train.csv')\n",
    "df_test = pd.read_csv('../data/unpreprocessed/test.csv')\n",
    "# Create label mappings\n",
    "unique_labels = df['label'].unique()\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# Create label statistics\n",
    "label_counts = df['label'].value_counts()\n",
    "print(\"train label counts: \", label_counts)\n",
    "label_counts = df_test['label'].value_counts()\n",
    "print(\"test label counts: \", label_counts)\n",
    "\n",
    "# Map labels to IDs\n",
    "df['label'] = df['label'].map(label2id)\n",
    "df_test['label'] = df_test['label'].map(label2id)\n",
    "\n",
    "# Rename columns for consistency\n",
    "df.columns = ['text', 'label']\n",
    "df_test.columns = ['public_id','text', 'label']\n",
    "\n",
    "# Shuffle and split the data\n",
    "df_train = df.sample(frac=1).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "#df_test = df_test.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model and Tokenizer Setup\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id  # Ensure padding token is set\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "use_8bit = False  # Set this to False if you want to use 4-bit quantization\n",
    "\n",
    "# Configure quantization\n",
    "if use_8bit:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True  # Load model in 8-bit\n",
    "    )\n",
    "else:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "# Load the causal language model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    quantization_config=quantization_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.eval()  # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Functions\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'balanced_accuracy': balanced_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def write_scores(test_df, csv_file, model_name, num_examples_per_label, quantization):\n",
    "    y_test = test_df.label\n",
    "    y_pred = test_df.predictions\n",
    "    \n",
    "    metrics = calculate_metrics(y_test, y_pred)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "    \n",
    "    # Concatenate num_examples_per_label and quantization to model_name\n",
    "    full_model_name = f\"{model_name.split('/')[-1]}-{num_examples_per_label}shot-{quantization}\"\n",
    "    \n",
    "    # Check if the file exists and has content\n",
    "    file_exists = os.path.isfile(csv_file) and os.path.getsize(csv_file) > 0\n",
    "    \n",
    "    # Open the file in append mode\n",
    "    with open(csv_file, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        \n",
    "        # Write header if the file is new or empty\n",
    "        if not file_exists:\n",
    "            writer.writerow(['Model', 'Metric', 'Value'])\n",
    "        \n",
    "        # Write results\n",
    "        writer.writerow([full_model_name, '', ''])  # Add model info\n",
    "        for metric, value in metrics.items():\n",
    "            writer.writerow(['', metric.capitalize(), f\"{value:.4f}\"])\n",
    "\n",
    "\n",
    "def generate_prompt(few_shot_examples, new_input):\n",
    "    prompt = \"\"\n",
    "    for example in few_shot_examples:\n",
    "        prompt += f\"Text: {example['text']}\\nLabel: {example['label']}\\n\\n\"\n",
    "    prompt += f\"Text: {new_input}\\nLabel:\"\n",
    "    return prompt\n",
    "\n",
    "def map_output_to_label(output_text, id2label):\n",
    "    # Extract the label from the generated text\n",
    "    # This assumes the model generates the label name directly after \"Label:\"\n",
    "    label = output_text.strip().split('\\n')[0]\n",
    "    # Find the label that best matches the generated text\n",
    "    for key in id2label.values():\n",
    "        if key.lower() in label.lower():\n",
    "            return key\n",
    "    return None  # Or a default label\n",
    "\n",
    "### Prepare Few-Shot Examples\n",
    "\n",
    "def get_few_shot_examples(df_train, num_examples_per_label=2):\n",
    "    few_shot = []\n",
    "    \n",
    "    # Calculate mean text length\n",
    "    mean_length = df_train['text'].str.len().mean()\n",
    "    \n",
    "    # Define a range around the mean (e.g., Â±20%)\n",
    "    lower_bound = mean_length * 0.8\n",
    "    upper_bound = mean_length * 1.2\n",
    "    \n",
    "    # For each label, select num_examples_per_label samples\n",
    "    for label_id in id2label.keys():\n",
    "        label_samples = df_train[df_train['label'] == label_id]\n",
    "        \n",
    "        # Filter samples within the desired length range\n",
    "        filtered_samples = label_samples[\n",
    "            (label_samples['text'].str.len() >= lower_bound) & \n",
    "            (label_samples['text'].str.len() <= upper_bound)\n",
    "        ]\n",
    "        \n",
    "        # If we have enough filtered samples, use them; otherwise, fall back to original samples\n",
    "        if len(filtered_samples) >= num_examples_per_label:\n",
    "            selected_samples = filtered_samples.sample(n=num_examples_per_label)\n",
    "        else:\n",
    "            selected_samples = label_samples.sample(n=num_examples_per_label)\n",
    "        \n",
    "        few_shot.extend(selected_samples.to_dict(orient='records'))\n",
    "    \n",
    "    # Shuffle the examples to mix labels\n",
    "    np.random.shuffle(few_shot)\n",
    "    \n",
    "    # Map label IDs back to label names\n",
    "    for example in few_shot:\n",
    "        example['label'] = id2label[example['label']]\n",
    "    \n",
    "    return few_shot\n",
    "\n",
    "### In-Context Learning Inference\n",
    "\n",
    "def in_context_predict(model, tokenizer, few_shot_examples, text):\n",
    "    prompt = generate_prompt(few_shot_examples, text)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate the output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1,  # Adjust based on expected label length\n",
    "            do_sample=False, # no need for randomness\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True\n",
    "        )\n",
    "    \n",
    "    # Decode the generated tokens\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the label\n",
    "    label = map_output_to_label(generated_text.split(\"Label:\")[-1], id2label)\n",
    "    return label, prompt, text\n",
    "\n",
    "\n",
    "### Save Model to Hugging Face Hub\n",
    "\n",
    "def save_model_to_hub(model, tokenizer, model_name, num_examples_per_label, quantization):\n",
    "    # Create a unique model name\n",
    "    hub_model_name = f\"{model_name.split('/')[-1]}-{num_examples_per_label}shot-{quantization}\"\n",
    "    \n",
    "    # Push the model to the hub\n",
    "    model.push_to_hub(hub_model_name, use_auth_token=True)\n",
    "    \n",
    "    # Push the tokenizer to the hub\n",
    "    tokenizer.push_to_hub(hub_model_name, use_auth_token=True)\n",
    "    \n",
    "    print(f\"Model and tokenizer saved to Hugging Face Hub as: {hub_model_name}\")\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "def evaluate_icl(model, tokenizer, df_test, few_shot_examples, id2label, label2id, model_name, num_examples_per_label, quantization):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    prompt_text_dict = {}\n",
    "    # Create a tqdm progress bar\n",
    "    progress_bar = tqdm(total=len(df_test), desc=\"Processing\", unit=\"sample\")\n",
    "    \n",
    "    for idx, row in df_test.iterrows():\n",
    "        text = row['text']\n",
    "        true_label = row['label']\n",
    "        \n",
    "        # Use in_context_predict function\n",
    "        predicted_label, prompt, text = in_context_predict(model, tokenizer, few_shot_examples, text)\n",
    "        \n",
    "        predicted_label_id = label2id.get(predicted_label, -1) if predicted_label is not None else -1\n",
    "        \n",
    "        y_true.append(true_label)\n",
    "        y_pred.append(predicted_label_id)\n",
    "        prompt_text_dict[idx] = {'prompt': prompt, 'text': text,'true_label': true_label, 'predicted_label_id': predicted_label_id}\n",
    "        \n",
    "        # Update the progress bar\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        # Clear the cache\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "    \n",
    "    # Add predictions to the dataframe\n",
    "    df_test = df_test.copy()\n",
    "    df_test['predictions'] = y_pred\n",
    "    \n",
    "    # Write scores to CSV\n",
    "    write_scores(df_test, 'few_shot_results.csv', model_name, num_examples_per_label, quantization)\n",
    "    \n",
    "    # Save the model to Hugging Face Hub\n",
    "    #save_model_to_hub(model, tokenizer, model_name, num_examples_per_label, quantization)\n",
    "    \n",
    "    return prompt_text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select few-shot examples\n",
    "num_examples_per_label = 3\n",
    "few_shot_examples = get_few_shot_examples(df_train, num_examples_per_label=num_examples_per_label)\n",
    "\n",
    "# Determine quantization\n",
    "quantization = \"8bit\" if quantization_config.load_in_8bit else \"4bit\"\n",
    "\n",
    "# Evaluate on the test set\n",
    "prompt_text_dict = evaluate_icl(model, tokenizer, df_test, few_shot_examples, id2label, label2id, model_name, num_examples_per_label, quantization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate predicted label id distribution\n",
    "label_distribution = {}\n",
    "label_mapping = {0: \"false\", 1: \"partially false\", 2: \"true\", 3: \"other\"}\n",
    "\n",
    "for item in prompt_text_dict.values():\n",
    "    predicted_label_id = item['predicted_label_id']\n",
    "    predicted_label_word = label_mapping[predicted_label_id]\n",
    "    \n",
    "    if predicted_label_word not in label_distribution:\n",
    "        label_distribution[predicted_label_word] = 0\n",
    "    label_distribution[predicted_label_word] += 1\n",
    "\n",
    "# Calculate percentages\n",
    "total_predictions = sum(label_distribution.values())\n",
    "label_distribution_percentage = {label: (count / total_predictions) * 100 for label, count in label_distribution.items()}\n",
    "\n",
    "# Print the distribution\n",
    "print(\"Predicted Label Distribution:\")\n",
    "for label, percentage in label_distribution_percentage.items():\n",
    "    print(f\"{label}: {percentage:.2f}%\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
